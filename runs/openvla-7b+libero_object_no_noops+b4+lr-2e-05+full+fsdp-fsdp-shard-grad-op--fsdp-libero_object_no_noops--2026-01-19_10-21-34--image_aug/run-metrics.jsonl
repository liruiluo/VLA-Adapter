{"hparams": {"adapter_tmp_dir": "adapter-tmp", "batch_size": 1, "config_file_path": "prismatic/extern/hf", "data_root_dir": "/datasets/zhangguoxi/modified_libero_rlds", "dataset_name": "libero_object_no_noops", "enable_gradient_checkpointing": false, "enable_mixed_precision_training": true, "epochs": null, "freeze_llm_backbone": false, "freeze_vision_backbone": false, "global_batch_size": 4, "grad_accumulation_steps": 4, "image_aug": true, "learning_rate": 2e-05, "lora_dropout": 0.0, "lora_rank": 32, "lr_scheduler_type": "constant", "max_grad_norm": 1.0, "max_steps": 5005, "num_images_in_input": 1, "per_device_batch_size": 1, "reduce_in_full_precision": false, "run_id_note": "fsdp-libero_object_no_noops--2026-01-19_10-21-34", "run_root_dir": "runs", "save_latest_checkpoint_only": true, "save_steps": 2500, "seed": 7, "shuffle_buffer_size": 100000, "trackers": ["jsonl", "wandb"], "train_strategy": "fsdp-shard-grad-op", "training_mode": "full", "unfreeze_last_llm_layer": true, "use_flash_attention_2": null, "use_fsdp": true, "use_hf_model": true, "use_lora": true, "use_quantization": false, "vla_path": "/models/zhangguoxi/openvla-7b", "wandb_entity": "yiyangchen-sylvia-bigai", "wandb_project": "openvla-fsdp", "warmup_ratio": 0.03, "weight_decay": 0.01}, "run_id": "openvla-7b+libero_object_no_noops+b4+lr-2e-05+full+fsdp-fsdp-shard-grad-op--fsdp-libero_object_no_noops--2026-01-19_10-21-34--image_aug"}
